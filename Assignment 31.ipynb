{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5c8bfe",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Min-Max scaling, also known as min-max normalization, is a data preprocessing technique used to scale and transform numerical features to a specific range, typically between 0 and 1. This method rescales the data, ensuring that the minimum value in the original dataset corresponds to 0, and the maximum value corresponds to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c543b5",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "Unit Vector scaling, also known as vector normalization, is a feature scaling technique used to transform the values of numerical features to a unit length (i.e., a length of 1) while preserving the direction or orientation of the data points. This method is particularly useful when the magnitude of the data points is not important, but their relative relationships or angles are significant. Unit Vector scaling is commonly applied to feature vectors in machine learning and data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b230c59",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in data analysis and machine learning. It's used to reduce the dimensionality of a dataset while preserving as much of the original variance as possible. PCA achieves this by finding linear combinations of the original features, called principal components, which capture the most important patterns and relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc5c57",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70e6dc1",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the field of dimensionality reduction and data analysis. PCA can be used as a technique for feature extraction, especially when you want to reduce the dimensionality of data while preserving the most important information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252834d3",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "we will apply min ax scaler formula on ach feature to scale them down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51874ea0",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "\n",
    "When working on a project to predict stock prices with a dataset that contains numerous features, using Principal Component Analysis (PCA) to reduce dimensionality can be a valuable technique. Here's how you would apply PCA to the dataset:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Start by cleaning and preprocessing the dataset. Handle missing data, normalize or standardize numerical features, and ensure that the data is in a suitable format for analysis.\n",
    "Feature Selection:\n",
    "\n",
    "Assess the relevance of each feature in the dataset for the task of predicting stock prices. Remove irrelevant or redundant features if possible. Feature engineering might also be applied to create new, informative features.\n",
    "Standardization:\n",
    "\n",
    "Standardize the features to have a mean of 0 and a standard deviation of 1. This step ensures that all features have the same scale, which is important for PCA.\n",
    "Covariance Matrix:\n",
    "\n",
    "Calculate the covariance matrix of the standardized features. The covariance matrix represents the relationships and variances between features. It's an essential step in PCA.\n",
    "Eigenvalue Decomposition:\n",
    "\n",
    "Perform eigenvalue decomposition on the covariance matrix. This step yields the eigenvalues and corresponding eigenvectors. The eigenvalues represent the amount of variance explained by each principal component, while the eigenvectors represent the direction or orientation of the principal components.\n",
    "Selecting Principal Components:\n",
    "\n",
    "Sort the eigenvalues in descending order and choose the top \n",
    "�\n",
    "k eigenvectors that capture a sufficient percentage of the total variance (e.g., 95% or 99%). This choice of \n",
    "�\n",
    "k determines the reduced dimensionality.\n",
    "Transform the Data:\n",
    "\n",
    "Project the original dataset onto the subspace defined by the selected \n",
    "�\n",
    "k principal components. This results in a new dataset with reduced dimensionality. Each data point now consists of \n",
    "�\n",
    "k principal component values instead of the original features.\n",
    "Model Building:\n",
    "\n",
    "Use the reduced-dimensional dataset as input for your stock price prediction model. The reduced features are expected to capture the most important patterns and relationships in the data while reducing the risk of overfitting due to high dimensionality.\n",
    "Model Evaluation and Fine-Tuning:\n",
    "\n",
    "Train and evaluate your stock price prediction model using the PCA-processed data. Continuously monitor the model's performance and adjust it as needed, considering the trade-off between dimensionality reduction and predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c1900",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9fa2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "data= [[1], [5], [10], [15], [20]]\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "scaled_data=scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e596e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585f4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
